#!/usr/bin/env python3
"""
COMPLETE SIZE-AWARE PIPELINE TEST RUN
=====================================
This script demonstrates the full size-aware VTON pipeline:
1. Load data with automatic size extraction
2. Encode size ratios to embeddings
3. Generate spatial size maps
4. Show how it integrates with UNet (simulated)

Run: python test_run_complete.py
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import torch
import numpy as np
from PIL import Image
import time

# Check GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\n{'='*70}")
print("SIZE-AWARE VTON PIPELINE - COMPLETE TEST RUN")
print(f"{'='*70}")
print(f"Device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
print(f"{'='*70}\n")

# ============================================================
# STEP 1: Load Size-Aware Modules
# ============================================================
print("STEP 1: Loading Size-Aware Modules...")
print("-" * 50)

from size_modules.size_encoder import SizeEncoder
from size_modules.size_controller import SimpleSizeController
from size_aware_dataset import SizeAwareVitonHDDataset
from torch.utils.data import DataLoader, Subset

# Initialize modules
size_encoder = SizeEncoder(input_dim=3, hidden_dim=256, output_dim=768).to(device)
size_controller = SimpleSizeController(size_embedding_dim=768, output_size=(128, 96)).to(device)

print(f"âœ… SizeEncoder loaded: {sum(p.numel() for p in size_encoder.parameters()):,} parameters")
print(f"âœ… SizeController loaded: {sum(p.numel() for p in size_controller.parameters()):,} parameters")

# ============================================================
# STEP 2: Load Dataset with Size Conditioning
# ============================================================
print(f"\n{'='*70}")
print("STEP 2: Loading Dataset with Automatic Size Extraction...")
print("-" * 50)

dataset = SizeAwareVitonHDDataset(
    dataroot_path="/scratch/bds9746/datasets/VITON-HD",
    phase="train",
    size=(512, 384),
    size_augmentation=False,  # Disable for clear demonstration
    enable_size_conditioning=True
)

print(f"âœ… Dataset loaded: {len(dataset)} samples")
print(f"   Size conditioning: ENABLED")
print(f"   Size augmentation: DISABLED (for clear demo)")

# Use 100 samples from VITON-HD
NUM_SAMPLES = 100
subset_indices = list(range(NUM_SAMPLES))
subset = Subset(dataset, subset_indices)
dataloader = DataLoader(subset, batch_size=4, shuffle=False, num_workers=2)

print(f"   Using {NUM_SAMPLES} samples from VITON-HD")

# ============================================================
# STEP 3: Process Samples Through Pipeline
# ============================================================
print(f"\n{'='*70}")
print("STEP 3: Processing Samples Through Size-Aware Pipeline")
print("="*70)

all_results = []
start_time = time.time()

print("\nProcessing 100 samples...")
print("(Showing details for first 5, then summary)\n")

for batch_idx, batch in enumerate(dataloader):
    # Get batch data
    images = batch['image']  # (B, 3, 512, 384)
    cloth = batch['cloth']  # (B, 1, 3, 224, 224) - CLIP processed
    cloth_pure = batch['cloth_pure']  # (B, 3, 512, 384) - full resolution
    size_ratios = batch['size_ratios'].to(device)  # (B, 3)
    size_labels = batch['size_label']  # List of strings
    size_label_ids = batch['size_label_id']  # (B,)

    batch_size = images.shape[0]

    # Encode sizes
    with torch.no_grad():
        size_embeddings = size_encoder(size_ratios)
        size_maps = size_controller(size_embeddings)

    # Show details for first few samples
    for i in range(batch_size):
        sample_idx = batch_idx * 4 + i
        ratios = size_ratios[i].cpu().numpy()
        label = size_labels[i]

        # Show detailed output for first 5 samples
        if sample_idx < 5:
            print(f"{'='*60}")
            print(f"ðŸ“¸ SAMPLE {sample_idx}")
            print(f"{'='*60}")
            print(f"\n  ðŸ“ SIZE EXTRACTION (Automatic from OpenPose + Warped Mask):")
            print(f"     Width Ratio:    {ratios[0]:.4f}  (garment_width / body_shoulder)")
            print(f"     Length Ratio:   {ratios[1]:.4f}  (garment_length / body_torso)")
            print(f"     Shoulder Ratio: {ratios[2]:.4f}  (garment_shoulder / body_shoulder)")
            print(f"     â†’ Classification: {label.upper()}")

            # Explain
            if label == 'tight':
                meaning = "Garment SMALLER than body â†’ form-fitting look"
            elif label == 'fitted':
                meaning = "Garment MATCHES body â†’ natural fit"
            elif label == 'loose':
                meaning = "Garment LARGER than body â†’ relaxed look"
            else:
                meaning = "Garment MUCH LARGER â†’ baggy/oversized look"
            print(f"     Meaning: {meaning}")

            print(f"\n  ðŸ§  SIZE ENCODING:")
            print(f"     Input:  [width, length, shoulder] = [{ratios[0]:.3f}, {ratios[1]:.3f}, {ratios[2]:.3f}]")
            print(f"     Output: 768-dimensional embedding")
            print(f"     Embedding norm: {size_embeddings[i].norm().item():.4f}")

            print(f"\n  ðŸ—ºï¸  SIZE MAP:")
            print(f"     Shape: (1, 128, 96) spatial guidance map")
            print(f"     Mean value: {size_maps[i].mean().item():.4f}")
            print(f"     This map tells the UNet WHERE to apply size awareness")

            print(f"\n  ðŸ”„ UNET INTEGRATION:")
            print(f"     â€¢ Size embedding â†’ Cross-attention (global size info)")
            print(f"     â€¢ Size map â†’ Self-attention (spatial guidance)")
            print(f"     â€¢ Result: Model generates {label} appearance\n")

        # Store results for all samples
        all_results.append({
            'index': sample_idx,
            'label': label,
            'ratios': ratios,
            'embedding_norm': size_embeddings[i].norm().item(),
            'map_mean': size_maps[i].mean().item()
        })

    # Progress update
    processed = (batch_idx + 1) * 4
    if processed % 20 == 0:
        print(f"  Processed {min(processed, NUM_SAMPLES)}/{NUM_SAMPLES} samples...")

elapsed = time.time() - start_time
print(f"\nâœ… Processed all {NUM_SAMPLES} samples in {elapsed:.2f} seconds")

# ============================================================
# STEP 7: Summary Statistics
# ============================================================
print(f"\n{'='*70}")
print("STEP 7: Summary Statistics")
print("="*70)

# Count by label
label_counts = {}
for r in all_results:
    label = r['label']
    label_counts[label] = label_counts.get(label, 0) + 1

print(f"\nSize Distribution in {NUM_SAMPLES} samples:")
for label in ['tight', 'fitted', 'loose', 'oversized']:
    count = label_counts.get(label, 0)
    pct = count / NUM_SAMPLES * 100
    bar = "â–ˆ" * int(pct / 5)
    print(f"  {label:10s}: {count:2d} ({pct:5.1f}%) {bar}")

# Ratio statistics
ratios_array = np.array([r['ratios'] for r in all_results])
print(f"\nRatio Statistics:")
print(f"  Width:    mean={ratios_array[:, 0].mean():.3f}, std={ratios_array[:, 0].std():.3f}")
print(f"  Length:   mean={ratios_array[:, 1].mean():.3f}, std={ratios_array[:, 1].std():.3f}")
print(f"  Shoulder: mean={ratios_array[:, 2].mean():.3f}, std={ratios_array[:, 2].std():.3f}")

# ============================================================
# STEP 8: Demonstrate Size Control
# ============================================================
print(f"\n{'='*70}")
print("STEP 8: Demonstrate Size Control (What Training Learns)")
print("="*70)

print("\nðŸŽ® MANUAL SIZE CONTROL DEMO:")
print("   You can manually set size ratios to control the output!\n")

test_sizes = [
    ([0.7, 0.8, 0.75], "TIGHT", "Form-fitting, stretched look"),
    ([1.0, 1.0, 1.0], "FITTED", "Natural, well-fitted look"),
    ([1.2, 1.15, 1.25], "LOOSE", "Relaxed, comfortable look"),
    ([1.5, 1.4, 1.6], "OVERSIZED", "Baggy, streetwear look"),
]

for ratios, label, description in test_sizes:
    ratio_tensor = torch.tensor([ratios], dtype=torch.float32).to(device)

    with torch.no_grad():
        embedding = size_encoder(ratio_tensor)
        size_map = size_controller(embedding)

    print(f"   Ratios: {ratios} â†’ {label}")
    print(f"   Description: {description}")
    print(f"   Embedding norm: {embedding.norm().item():.4f}")
    print(f"   Size map mean: {size_map.mean().item():.4f}")
    print()

# ============================================================
# FINAL SUMMARY
# ============================================================
print(f"{'='*70}")
print("âœ… COMPLETE PIPELINE TEST SUCCESSFUL!")
print("="*70)

print("""
WHAT WE DEMONSTRATED:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. âœ… Automatic size extraction from images (OpenPose + warped masks)
2. âœ… Size ratio computation (garment/body dimensions)
3. âœ… Size classification (tight/fitted/loose/oversized)
4. âœ… Size encoding to 768-dim embeddings
5. âœ… Spatial size map generation (128Ã—96)
6. âœ… Integration points for UNet conditioning

HOW THIS SOLVES THE SIZING PROBLEM:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BEFORE: Model ignores size â†’ XL and XS look the same
AFTER:  Model receives size info â†’ XL looks baggy, XS looks tight

THE KEY INSIGHT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Size ratios tell the model HOW MUCH bigger/smaller the garment is
â€¢ Size embeddings inject this into cross-attention (global conditioning)
â€¢ Size maps inject this into self-attention (spatial conditioning)
â€¢ Model learns: ratio > 1.3 = generate baggier output

READY FOR TRAINING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
All modules working! Next: Run actual training to teach the model
to generate different outputs based on size conditioning.
""")

print(f"{'='*70}")
print("Test completed successfully! ðŸŽ‰")
print(f"{'='*70}\n")
